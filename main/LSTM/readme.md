В этой папке лежит моя первая программа классификации, которую я делал с очень серьёзным лицом.

# char_classifier_trainer.py
Этот файл содержит полную систему обучения и валидации LSTM-классификатора.  
Для запуска нужно только указать файл с данными.

### Настройка
В начале кода есть блок с параметрами, с которыми можно поиграться. Кратко о главных из них:
- `MAX_VOCAB_SIZE` — максимальный размер словаря. Если текстов много, можно ограничить частотными символами, чтобы не раздувать модель.
- `MAX_SEQ_LENGTH` — максимальная длина строки (в символах). Всё длиннее просто обрезается.
- `EMBEDDING_DIM` — размерность вектора для каждого символа. Чем больше — тем точнее различия, но тем выше нагрузка на память.
- `HIDDEN_DIM` — размер скрытого состояния LSTM, то есть сколько информации модель «помнит» о контексте. Больше — глубже понимание, но медленнее обучение.
- `NUM_LAYERS` — количество слоёв LSTM. Один слой — проще и быстрее, два и больше — точнее, но тяжелее.
- `BATCH_SIZE` — сколько примеров обрабатывается за один шаг. Больше — быстрее, меньше — стабильнее.
- `LEARNING_RATE` — скорость обучения. Если слишком высокая — модель будет «скакать», если слишком низкая — почти не будет учиться.
- `NUM_EPOCHS` — сколько раз модель проходит через все данные. Обычно 3–5 хватает, чтобы понять, работает ли вообще.
- `TEST_SIZE` — доля данных, откладываемая для проверки точности (валидации). 0.2 = 20% данных идёт в тест.
- `MIN_SAMPLES` — минимальное число примеров на класс. Классы с меньшим количеством просто игнорируются, чтобы не мешали обучению.

### Немного о данных
Для обучения я использовал экспорт с Tatoeba.  
Файл имел расширение `.csv`, но внутри был разделён табуляцией (как в `.tsv`), поэтому в коде деление идёт именно по табуляции.  
Первый столбец — метка класса, второй — текст.  

Из-за того, что в Tatoeba встречаются классы с очень малым количеством примеров, я добавил параметр `MIN_SAMPLES`. Он позволяет исключать такие классы, чтобы модель не «спотыкалась» на них.

### Немного о сохранении
Словарь сохраняется **вместе с моделью**, поэтому параметр `VOCAB_SAVE_PATH` в коде технически не используется.  
Пример того, как достать словарь из сохранённой модели, показан в файле `using.py`.

# using.py
Этот файл отвечает за использование обученных моделей.  
Он умеет загружать модель и классифицировать тексты.  

Тексты при этом тоже обрезаются до заданной длины — это можно убрать, но лучше не стоит, если модель обучалась с обрезкой.

Есть два режима:
1. **Интерактивный** — можно ввести текст вручную, и программа покажет до 10 самых вероятных классов.
2. **Пакетный** — можно загрузить CSV-файл:
   - либо просто список строк (одна строка = один текст),
   - либо тот же формат, что при обучении (`класс<TAB>текст`), чтобы программа могла сравнить реальные метки с предсказаниями.

# dataset_editor.html
Этот файл — GUI для удобного редактирования наборов данных. Он использует тот же формат, что и остальной проект. В нём отображается некоторая статистика по набору, можно добавлять и искать записи.
Могут быть проблемы со слишком большими наборами данных.
Данные в этом GUI хранятся в памяти браузера, поэтому страницу можно перезагружать и закрывать не переживая за данные (но лучше перестраховаться и экспортировать перед закрытием).
Импорт
